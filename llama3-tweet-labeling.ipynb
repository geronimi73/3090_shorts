{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d53b595-a34d-44ab-badc-c3a4fa83dff8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datasets==2.19.0\n",
      "sentence-transformers==2.3.1\n",
      "torch==2.2.1\n",
      "torch-grammar==0.3.3\n",
      "transformers==4.40.1\n"
     ]
    }
   ],
   "source": [
    "!pip freeze | egrep -w \"transformers|datasets|torch\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "015eee16-8c0b-47c3-b96f-723c1162078d",
   "metadata": {},
   "source": [
    "# Load model and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "424d4ea0-180d-4c92-946a-c687abe96196",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "modelpath=\"models/llama3-8b\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    modelpath,    \n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(modelpath, use_fast=False) \n",
    "\n",
    "def generate(prompt, max_new_tokens = 100): \n",
    "    prompt_tokenized=tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "    output_tokenized = model.generate(\n",
    "        **prompt_tokenized, \n",
    "        max_new_tokens = max_new_tokens,\n",
    "        pad_token_id = tokenizer.eos_token_id)[0]\n",
    "    \n",
    "    output_tokenized = output_tokenized[len(prompt_tokenized[\"input_ids\"][0]):]\n",
    "    output = tokenizer.decode(output_tokenized)\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d3c2a8f-b97f-4185-892d-ca28cd218fd0",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "https://huggingface.co/datasets/m-newhauser/senator-tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc73f15-60c4-4c85-a6e0-3455fd17084c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import json\n",
    "\n",
    "ds = load_dataset(\"m-newhauser/senator-tweets\")\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35247b66-6308-4ef7-8db1-3ce18137d121",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "544fbe0c-2c2b-4e2b-b443-506cc0b1f0b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ds.remove_columns(\"embeddings\")\n",
    "\n",
    "print(json.dumps(ds[\"train\"][0], indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e81aca96-6f68-4b4b-9ee4-b5c49bc95174",
   "metadata": {},
   "source": [
    "# Define answer scheme, few-shot prompts and prompt for a single entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad9f2cd-5cd0-46a9-bd30-0afd2948e409",
   "metadata": {},
   "outputs": [],
   "source": [
    "def few_shot_prompt(dataset):\n",
    "    few_shot_prompt = []\n",
    "    \n",
    "    for entry in dataset:\n",
    "        few_shot_prompt.append(\"\\nInput:\")\n",
    "        few_shot_prompt.append(json.dumps(dict(text=entry[\"text\"]),indent = 2))\n",
    "        few_shot_prompt.append(\"Analysis:\")\n",
    "        few_shot_prompt.append(json.dumps(dict(political_party=entry[\"party\"]),indent = 2))\n",
    "\n",
    "    return \"\\n\".join(few_shot_prompt).strip()\n",
    "\n",
    "print(few_shot_prompt(ds[\"train\"].select(range(3))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2329c79c-cbb4-4920-8873-7558821d85be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def entry_to_prompt(entry):\n",
    "    prompt = []\n",
    "    prompt.append(\"Input:\")\n",
    "    prompt.append(json.dumps(dict(text=entry[\"text\"]),indent = 2))\n",
    "    prompt.append(\"Analysis:\")\n",
    "\n",
    "    return \"\\n\".join(prompt).strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "632685e1-7981-4efd-8f08-fea126751c63",
   "metadata": {},
   "source": [
    "## 1. Predict political party of tweet author"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "448ce1c4-18bb-4f35-990c-aeea758b7e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5 shot prompting\n",
    "examples = few_shot_prompt(ds[\"train\"].select(range(5)))\n",
    "\n",
    "# test llama on 100 tweets\n",
    "dataset = ds[\"train\"].select(range(11, 111))\n",
    "\n",
    "total, correct, unknown = 0, 0, 0\n",
    "\n",
    "for i, entry in enumerate(dataset):\n",
    "    prompt = examples + \"\\n\\n\" + entry_to_prompt(entry) + \"\\n\"\n",
    "    if i==0:\n",
    "        print(\"#\" * 30,f\"\\n{prompt}\\n\",\"#\" * 30)\n",
    "    \n",
    "    answer = generate(prompt)\n",
    "    print(f\"\\nTweet {i+1} (id {entry['id']}): '{entry['text']}'\")\n",
    "\n",
    "    # Cut answer at the point where llama hallucinates the next tweet\n",
    "    answer = answer.split(\"Input:\")[0] if \"Input:\" in answer else answer\n",
    "    print(answer)\n",
    "\n",
    "    # parse JSON\n",
    "    answer_parsed = json.loads(answer)\n",
    "    answer_party = answer_parsed[\"political_party\"].lower()\n",
    "\n",
    "    # count and output num. of correct and incorrect answers \n",
    "    is_correct = answer_party == entry[\"party\"].lower()\n",
    "    correct += 1 if is_correct else 0\n",
    "    unknown += 1 if answer_party ==\"unknown\" else 0\n",
    "    total += 1\n",
    "    print(\" *\",f\"Prediction ({answer_party}) is\",\"correct\" if is_correct else f\"WRONG ({entry['party']} is the correct answer)\")\n",
    "    print(\" *\",\"Stats: \", f\"{round(correct/total*100,2)}% correct, ({total} total, {correct} correct, {unknown} unknown)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bfba41e-3936-4050-9f7c-25f4882ea499",
   "metadata": {},
   "source": [
    "### 5 shot prompting\n",
    "**with analysis scheme** in prompt:\n",
    "* Stats:  90.0% correct, (100 total, 90 correct, 0 unknown)\n",
    "\n",
    "**without analysis scheme** in prompt:\n",
    "* Stats:  91.0% correct, (100 total, 91 correct, 0 unknown)\n",
    "\n",
    "### 10 shot prompting\n",
    "**without analysis scheme** in prompt:\n",
    "* Stats:  86.0% correct, (100 total, 86 correct, 0 unknown)\n",
    "* Stats:  89.0% correct, (100 total, 89 correct, 0 unknown)\n",
    "Selection deleted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3652cf8c-8bc6-4306-9bd6-495946d824d0",
   "metadata": {},
   "source": [
    "## 2. Extract topic, entities, keywords from tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb9e114a-8e3d-4e96-b300-83c7330f8739",
   "metadata": {},
   "source": [
    "### Semi-manual curated 5-shot prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c20a325-04b0-452d-80e8-0d3d8149d053",
   "metadata": {},
   "outputs": [],
   "source": [
    "examples=\"\"\"\n",
    "Input:\n",
    "{\n",
    "  \"text\": \"The crisis and chaos on the southern border are inexcusable. This was avoidable, but the Biden administration's open border policies and disastrous refusal to secure the border are responsible for this failure. https://t.co/7IwKDYdmTo\"\n",
    "}\n",
    "Analysis:\n",
    "{\n",
    "  \"party\": \"Republican\",\n",
    "  \"topic\": \"Border Crisis\",\n",
    "  \"actors\": [\"Biden\"],\n",
    "  \"location\": [\"southern border\"],\n",
    "  \"strong_words\": [\"crisis\", \"chaos\", \"inexcusable\", \"disastrous refusal\", \"failure\"],\n",
    "  \"hashtags\": [\"#BorderChaos\", \"#BidenBorderDisaster\", \"#FailedImmigrationPolicy\"]\n",
    "}\n",
    "\n",
    "Input:\n",
    "{\n",
    "  \"text\": \"All nations should condemn the Chinese Communist Party's human rights abuses against its ethnic minorities, including Uyghurs and Tibetan people. We must hold the Chinese Communist Party accountable for these atrocities. https://t.co/11ovy05Ilp\"\n",
    "}\n",
    "Analysis:\n",
    "{\n",
    "  \"party\": \"Democrat\",\n",
    "  \"topic\": \"Human Rights Abuses\",\n",
    "  \"actors\": [\"Chinese Communist Party\", \"Uyghurs and Tibetan people\"],\n",
    "  \"location\": [ ],\n",
    "  \"strong_words\": [\"abuses\", \"accountable\", \"atrocities\"],\n",
    "  \"hashtags\": [\"#CCPHumanRightsAbuse\", \"#UyghurGenocide\", \"#TibetanOppression\"]\n",
    "}\n",
    "\n",
    "Input:\n",
    "{\n",
    "  \"text\": \"Congress enacted two bipartisan sanctions laws to stop Putin's Nord Stream pipeline, but @POTUS is giving up and  hiding behind ridiculous talking points. If Biden Admin actually imposed sanctions that it's waiving, that would halt #NS2's completion for foreseeable future. https://t.co/M5TISJBpEX\"\n",
    "}\n",
    "Analysis:\n",
    "{\n",
    "  \"party\": \"Republican\",\n",
    "  \"topic\": \"Nord Stream Sanctions\",\n",
    "  \"actors\": [\"Congress\", \"Putin\", \"POTUS\", \"Biden\"],\n",
    "  \"location\": [ ],\n",
    "  \"strong_words\": [\"giving up\", \"ridiculous\"],\n",
    "  \"hashtags\": [\"#BidenCavesToPutin\", \"#NordStreamSanctions\", \"#FailedForeignPolicy\"]\n",
    "}\n",
    "\n",
    "Input:\n",
    "{\n",
    "  \"text\": \"I'm introducing a bill with @SenRubioPress and @SenatorMenendez to help end violence against women and children in El Salvador, Honduras and Guatemala and provide victims with the support and shelter they need. More: https://t.co/OBpqOXsioQ\"\n",
    "}\n",
    "Analysis:\n",
    "{\n",
    "  \"party\": \"Democrat\",\n",
    "  \"topic\": \"Ending Violence\",\n",
    "  \"actors\": [\"SenRubioPress\", \"SenatorMenendez\", \"women\", \"children\"],\n",
    "  \"location\": [\"El Salvador\", \"Honduras\", \"Guatemala\"],\n",
    "  \"strong_words\": [\"end violence\", \"support\", \"shelter\"],\n",
    "  \"hashtags\": [\"#EndGenderViolence\", \"#CentralAmericaAid\", \"#SupportSurvivors\"]\n",
    "}\n",
    "\n",
    "Input:\n",
    "{\n",
    "  \"text\": \"Kelley and I are heartbroken at the news of the tragic death of the FBI agents today. Our prayers go out to their families during this time. https://t.co/YeDilcKgL8 https://t.co/We2FgfOP62\"\n",
    "}\n",
    "Analysis:\n",
    "{\n",
    "  \"party\": \"Republican\",\n",
    "  \"topic\": \"FBI Agents Deaths\",\n",
    "  \"actors\": [\"Kelley\", \"FBI agents\"],\n",
    "  \"location\": [ ],\n",
    "  \"strong_words\": [\"heartbroken\", \"tragic death\", \"prayers\"],\n",
    "  \"hashtags\": [\"#FBITragedy\", \"#PrayersForFamilies\", \"#LawEnforcementSacrifice\"]\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4ff295a-3b96-41ba-aa22-cc347bc318fa",
   "metadata": {},
   "source": [
    "### Prompt llama again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b31bdc8-effc-44ad-a4be-c037eceac443",
   "metadata": {},
   "outputs": [],
   "source": [
    "total, correct, unknown = 0, 0, 0\n",
    "\n",
    "dataset = ds[\"train\"].select(range(11, 111))\n",
    "\n",
    "for i, entry in enumerate(dataset):\n",
    "    print(f\"\\nTweet {i+1} (id {entry['id']}): '{entry['text']}'\")\n",
    "\n",
    "    prompt = examples + \"\\n\\n\" + entry_to_prompt(entry) + \"\\n\"\n",
    "    answer = generate(prompt, max_new_tokens=250)\n",
    "    answer = answer.split(\"Input:\")[0].strip() if \"Input:\" in answer else answer.strip()\n",
    "    \n",
    "    print(answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
