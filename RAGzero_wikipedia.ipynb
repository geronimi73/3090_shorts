{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9e009e17-6e4c-4b62-bec0-2ac1473c1856",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install wikipedia-api huggingface_hub lancedb pylance pydantic tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9357d47b-9124-4da1-976a-5295c2848bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup a HF API Key at https://huggingface.co/settings/tokens\n",
    "HF_API_KEY = \"XXXXXXXXXX\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "32eb5264-12f5-4f3d-8b84-b69a53c2c5cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipediaapi\n",
    "import requests\n",
    "import lancedb\n",
    "from tqdm import tqdm\n",
    "from huggingface_hub import InferenceClient\n",
    "from lancedb.pydantic import LanceModel, Vector\n",
    "\n",
    "# scrape wikipedia\n",
    "def get_wiki_article(title):\n",
    "    # wikipedia wants this for whatever reason\n",
    "    user_agent = 'YourAppName/1.0 (YourContactInfo)'\n",
    "    wiki_wiki = wikipediaapi.Wikipedia(user_agent=user_agent, language=\"en\")\n",
    "    return wiki_wiki.page(title).text\n",
    "\n",
    "# chunk a string into max chunk_size words\n",
    "def chunk_text(text, chunk_size=500):\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "\n",
    "    for i in range(0, len(words), chunk_size):\n",
    "        chunk = words[i:i + chunk_size]\n",
    "        chunks.append(' '.join(chunk))\n",
    "\n",
    "    return chunks\n",
    "\n",
    "# embed a list of strings\n",
    "def embed(text_items, batch_size=8):\n",
    "    # always pass a list of strings\n",
    "    text_items = [text_items] if isinstance(text_items, str) else text_items\n",
    "    \n",
    "    hf_client = InferenceClient(api_key=HF_API_KEY)\n",
    "\n",
    "    # Process text items in batches\n",
    "    embeddings = []\n",
    "    for i in tqdm(\n",
    "        range(0, len(text_items), batch_size),\n",
    "        \"Embedding..\"\n",
    "    ):\n",
    "        batch = text_items[i:i + batch_size]\n",
    "        batch_embeddings = hf_client.feature_extraction(\n",
    "            text=batch, model=\"mixedbread-ai/mxbai-embed-large-v1\"\n",
    "        )\n",
    "        embeddings.extend(batch_embeddings)\n",
    "\n",
    "    return embeddings\n",
    "\n",
    "# search vector store for query (str)\n",
    "def retrieve(query):\n",
    "    query = embed(query)\n",
    "    \n",
    "    return table.search(query).limit(5).to_pydantic(TextChunk)\n",
    "\n",
    "question = \"How many Turkish people live in Paris?\"\n",
    "\n",
    "# ask LLM providing chunks as context\n",
    "def generate(question, retrieved_chunks):\n",
    "    msg_template = \"\"\"Please answer the question using only the provided Wikipedia excerpts.\n",
    "    \n",
    "    Question: {question}\n",
    "    \n",
    "    Relevant Wikipedia excerpts:\n",
    "    {chunks}\"\"\"\n",
    "    \n",
    "    msg = msg_template.format(\n",
    "        question=question,\n",
    "        chunks=\"\\n\\n\".join([f\"Excerpt #{i+1}: {result.text}\" for i, result in enumerate(retrieved_chunks)])\n",
    "    )\n",
    "    \n",
    "    hf_client = InferenceClient(api_key=HF_API_KEY)    \n",
    "    output = hf_client.chat_completion(\n",
    "        model=\"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "        messages=[\n",
    "            dict(role=\"system\", content=\"You are a helpful assistant.\"),\n",
    "            dict(role=\"user\", content=msg),\n",
    "        ],\n",
    "        max_tokens=2048,\n",
    "    )\n",
    "    \n",
    "    return \" \".join([choice.message.content for choice in output.choices])\n",
    "\n",
    "class TextChunk(LanceModel):\n",
    "    # emb. dimension of mxbai-embed-large-v1 = 1024\n",
    "    vector: Vector(1024)\n",
    "    text: str \n",
    "    doc_title: str\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "4a6320c6-5367-449e-9d3a-98778e4ee3b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding..: 100%|████████████████████████████████| 4/4 [00:11<00:00,  2.95s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AddResult(version=2)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki_article = \"Paris\"\n",
    "\n",
    "# fetch wikipedia article, chunk and embed\n",
    "text = get_wiki_article(wiki_article)\n",
    "text_chunks = chunk_text(text)\n",
    "text_embeddings = embed(text_chunks)\n",
    "\n",
    "# ready chunks for DB\n",
    "lance_chunks = [\n",
    "    dict(vector=emb, text=text, doc_title=wiki_article)\n",
    "    for emb, text in zip(text_embeddings, text_chunks)\n",
    "]\n",
    "\n",
    "# store in vector DB\n",
    "db = lancedb.connect(\"vectorstore\") # can be any name, will create a directory where vectors are stored\n",
    "table = db.create_table(\"wiki_docs\", schema=TextChunk, exist_ok=True)\n",
    "table.add(lance_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "2e27412a-0e13-4e08-a8bf-73a7c79a7062",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding..: 100%|████████████████████████████████| 1/1 [00:00<00:00,  3.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To find where most immigrants in Paris are from, we need to analyze the provided Wikipedia excerpts. Excerpt #1 mentions the immigrant population in the City of Paris and the Paris Region in 2012:\n",
      "\n",
      "- In the City of Paris: \n",
      "  - Europe: 135,853\n",
      "  - Maghreb: 112,369\n",
      "  - Sub-Saharan Africa and Egypt: 70,852\n",
      "  - Turkey: 5,059\n",
      "  - Asia outside Turkey: 91,297\n",
      "  - Americas: 38,858\n",
      "  - South Pacific: 1,365\n",
      "\n",
      "- In the Paris Region:\n",
      "  - Europe: 590,504\n",
      "  - Maghreb: 627,078\n",
      "  - Sub-Saharan Africa and Egypt: 435,339\n",
      "  - Turkey: 69,338\n",
      "  - Asia outside Turkey: 322,330\n",
      "  - Americas: 113,363\n",
      "  - South Pacific: 2,261\n",
      "\n",
      "From the above data, it's clear that the majority of immigrants in Paris are from Europe and the Maghreb region.\n"
     ]
    }
   ],
   "source": [
    "# retrieve and generate \n",
    "question = \"where are most immigrants in Paris from?\"\n",
    "\n",
    "retrieved_chunks = retrieve(question)\n",
    "answer = generate(question, retrieved_chunks)\n",
    "\n",
    "print(answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
