{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install essential libraries for fine-tuning from the Granite Vision notebook\n",
    "# !pip install git+https://github.com/huggingface/transformers.git\n",
    "# !pip install -U trl datasets bitsandbytes peft accelerate trackio\n",
    "\n",
    "# Install docling to handle document processing and DocTags conversion\n",
    "# !pip install docling\n",
    "\n",
    "# Optional: Install Flash Attention for better performance on compatible GPUs\n",
    "# !pip install flash-attn --no-build-isolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VhY4_IMhoVoL"
   },
   "outputs": [],
   "source": [
    "# from huggingface_hub import login\n",
    "# from kaggle_secrets import UserSecretsClient\n",
    "\n",
    "# user_secrets = UserSecretsClient()\n",
    "# secret_value_0 = user_secrets.get_secret(\"HF_TOKEN\")\n",
    "# login(token=secret_value_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LpeiFgYLBRCi",
    "outputId": "d3bb089d-a8cd-43e3-bb24-56bad00a91e5"
   },
   "outputs": [],
   "source": [
    "# Optional: Install Flash Attention for better performance on compatible GPUs\n",
    "# !pip install -q flash-attn --no-build-isolation\n",
    "try:\n",
    "    import flash_attn\n",
    "    print(\"FlashAttention is installed.\")\n",
    "    USE_FLASH_ATTENTION = True\n",
    "except ImportError:\n",
    "    print(\"FlashAttention is not installed.\")\n",
    "    USE_FLASH_ATTENTION = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ujtItcBIDZIT"
   },
   "source": [
    "# Get the Dataset and convert to doctags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "df_8cIntLPyK"
   },
   "outputs": [],
   "source": [
    "# # Run this command in a Colab cell\n",
    "# !rm -rf ~/.cache/huggingface/datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uRhkdy8JDdCe"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from docling_core.types.doc import DoclingDocument, BoundingBox, ProvenanceItem, PageItem, PictureItem, ImageRef, Size, DocItemLabel\n",
    "from PIL import Image\n",
    "import io\n",
    "import json\n",
    "# 1. Load the docling-dpbench dataset\n",
    "dataset_id = \"ds4sd/docling-dpbench\"\n",
    "# Let's use the 'default' configuration, train split\n",
    "dataset = load_dataset(dataset_id, name=\"default\", split=\"test\")\n",
    "\n",
    "# For a real scenario, you would use the full dataset and split it\n",
    "# For this demonstration, we'll just use a small subset\n",
    "if len(dataset) > 120:\n",
    "    train_dataset_raw = dataset.select(range(100))\n",
    "    test_dataset_raw = dataset.select(range(100, 120))\n",
    "else:\n",
    "    # Handle smaller datasets by splitting what's available\n",
    "    train_test_split = dataset.train_test_split(test_size=0.1, seed=42)\n",
    "    train_dataset_raw = train_test_split[\"train\"]\n",
    "    test_dataset_raw = train_test_split[\"test\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uVkrZMGrHOZc",
    "outputId": "ca4ae50e-1f6f-466f-e6a3-0061256322e6"
   },
   "outputs": [],
   "source": [
    "\n",
    "# 2. Define the system message and user prompt\n",
    "system_message = \"A chat between a user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions.\"\n",
    "user_prompt = \"Convert this page to docling.\" # This is a supported instruction [7]\n",
    "\n",
    "def convert_to_rgb(image):\n",
    "    \"\"\"Convert image to RGB format if not already in RGB.\"\"\"\n",
    "    if image.mode == \"RGB\":\n",
    "        return image\n",
    "    image_rgba = image.convert(\"RGBA\")\n",
    "    background = Image.new(\"RGBA\", image_rgba.size, (255, 255, 255))\n",
    "    alpha_composite = Image.alpha_composite(background, image_rgba)\n",
    "    return alpha_composite.convert(\"RGB\")\n",
    "\n",
    "\n",
    "def reduce_image_size(image, scale=0.5):\n",
    "    \"\"\"Reduce image size by a given scale.\"\"\"\n",
    "    original_width, original_height = image.size\n",
    "    new_width = int(original_width * scale)\n",
    "    new_height = int(original_height * scale)\n",
    "    return image.resize((new_width, new_height))\n",
    "\n",
    "# 3. Function to process a docling-dpbench sample\n",
    "def process_dpbench_sample(sample):\n",
    "    \"\"\"\n",
    "    Processes a sample from the dpbench dataset to extract the image\n",
    "    and the target DocTags string.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load the ground truth image from bytes\n",
    "        # The image data is in a list, we'll take the first one\n",
    "        image_bytes_data = sample[\"GroundTruthPageImages\"][0]['bytes']\n",
    "        image = Image.open(io.BytesIO(image_bytes_data))\n",
    "\n",
    "        # Load the ground truth DoclingDocument from its JSON string representation\n",
    "        doc_json_str = sample[\"GroundTruthDocument\"]\n",
    "        doc_dict = json.loads(doc_json_str)\n",
    "        doc = DoclingDocument(**doc_dict)\n",
    "\n",
    "        # Export the document to the required DocTags format [8-10]\n",
    "        # This is the target output for the model\n",
    "        target_doctags = doc.export_to_doctags()\n",
    "\n",
    "        return {\n",
    "            \"image\": image,\n",
    "            \"target_text\": target_doctags\n",
    "        }\n",
    "    except Exception as e:\n",
    "        # If a sample is corrupted or fails processing, we skip it\n",
    "        print(f\"Skipping sample due to error: {e}\")\n",
    "        return None\n",
    "\n",
    "# 4. Format the processed data into the required chat structure [11]\n",
    "def format_data(processed_sample):\n",
    "    \"\"\"\n",
    "    Formats the processed data (image and target text) into the chat\n",
    "    template expected by the model and trainer.\n",
    "    \"\"\"\n",
    "    image = processed_sample[\"image\"]\n",
    "    image = convert_to_rgb(image)\n",
    "    image = reduce_image_size(image)\n",
    "    return [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": [{\"type\": \"text\", \"text\": system_message}],\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"image\",\n",
    "                    \"image\": image,\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": user_prompt,\n",
    "                },\n",
    "            ],\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": [{\"type\": \"text\", \"text\": processed_sample[\"target_text\"]}],\n",
    "        },\n",
    "    ]\n",
    "\n",
    "# Process the raw datasets\n",
    "processed_train = [process_dpbench_sample(s) for s in train_dataset_raw]\n",
    "processed_test = [process_dpbench_sample(s) for s in test_dataset_raw]\n",
    "\n",
    "# Filter out any samples that failed processing and format them\n",
    "train_dataset = [format_data(p) for p in processed_train if p is not None]\n",
    "test_dataset = [format_data(p) for p in processed_test if p is not None]\n",
    "\n",
    "print(f\"Successfully processed {len(train_dataset)} training samples.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U7ySCXmHoVoS",
    "outputId": "85b092a3-ea65-4cc4-e0bd-f7ad416be9b4"
   },
   "outputs": [],
   "source": [
    "# print(train_dataset[12])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "91LZ7CZWoVoT"
   },
   "source": [
    "# Loading the model and the tokenizer and testing them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gVk5BXajoVoU"
   },
   "outputs": [],
   "source": [
    "# from transformers import AutoModelForVision2Seq, AutoProcessor, BitsAndBytesConfig\n",
    "# model_id = \"ibm-granite/granite-docling-258M\"\n",
    "# model = AutoModelForVision2Seq.from_pretrained(\n",
    "#     model_id,\n",
    "#     device_map='cuda',\n",
    "#     torch_dtype=torch.float16,\n",
    "# )\n",
    "# processor = AutoProcessor.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KSu5qOsqoVoV"
   },
   "outputs": [],
   "source": [
    "# import time\n",
    "# import torch\n",
    "# from statistics import mean\n",
    "\n",
    "# def measure_generation_speed(model, processor, sample,\n",
    "#                              device='cuda',\n",
    "#                              max_new_tokens=128,\n",
    "#                              warmup_runs=1,\n",
    "#                              runs=1):\n",
    "#     \"\"\"\n",
    "#     Returns: dict with keys:\n",
    "#       - ttf_s: time-to-first-token (seconds, generate-only)\n",
    "#       - avg_gen_time_s: average generate() call time (seconds)\n",
    "#       - tokens_per_sec: tokens/sec computed as (actual_generated_tokens / avg_gen_time_s)\n",
    "#       - tokens_generated: actual generated tokens observed (per sample)\n",
    "#     \"\"\"\n",
    "#     # resolve device\n",
    "#     if device is None:\n",
    "#         try:\n",
    "#             device = str(model.device)\n",
    "#         except Exception:\n",
    "#             device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "#     device = torch.device(device)\n",
    "\n",
    "#     # helper for accurate timing with CUDA\n",
    "#     def _sync():\n",
    "#         if device.type == \"cuda\":\n",
    "#             torch.cuda.synchronize()\n",
    "\n",
    "#     # prepare tokenized inputs (on CPU)\n",
    "#     proc_out = processor.apply_chat_template(\n",
    "#         [sample[\"messages\"][1]],\n",
    "#         add_generation_prompt=True,\n",
    "#         tokenize=True,\n",
    "#         return_dict=True,\n",
    "#         return_tensors=\"pt\",\n",
    "#     )\n",
    "#     input_len = proc_out[\"input_ids\"].shape[-1]\n",
    "\n",
    "#     # move tensors to device\n",
    "#     batch_on_device = {k: (v.to(device) if isinstance(v, torch.Tensor) else v)\n",
    "#                        for k, v in proc_out.items()}\n",
    "\n",
    "#     # warmup\n",
    "#     # model.to(device)\n",
    "#     # model.eval()\n",
    "#     with torch.no_grad():\n",
    "#         for _ in range(warmup_runs):\n",
    "#             _sync()\n",
    "#             _ = model.generate(**batch_on_device, max_new_tokens=min(8, max_new_tokens))\n",
    "#             _sync()\n",
    "\n",
    "#     # 1) Time-to-first-token (generate-only): single call with max_new_tokens=1\n",
    "#     _sync()\n",
    "#     t0 = time.perf_counter()\n",
    "#     with torch.no_grad():\n",
    "#         out = model.generate(**batch_on_device, max_new_tokens=1)\n",
    "#     _sync()\n",
    "#     ttf = time.perf_counter() - t0\n",
    "\n",
    "#     # compute how many new tokens were produced in that call (usually 1)\n",
    "#     first_generated = out[0].shape[-1] - input_len\n",
    "\n",
    "#     # 2) Tokens/sec (generation-only): run several full generate calls and average\n",
    "#     gen_times = []\n",
    "#     observed_generated = None\n",
    "#     with torch.no_grad():\n",
    "#         for _ in range(runs):\n",
    "#             _sync()\n",
    "#             tstart = time.perf_counter()\n",
    "#             out = model.generate(**batch_on_device, max_new_tokens=max_new_tokens)\n",
    "#             _sync()\n",
    "#             elapsed = time.perf_counter() - tstart\n",
    "#             gen_times.append(elapsed)\n",
    "#             generated = out[0].shape[-1] - input_len\n",
    "#             observed_generated = generated if observed_generated is None else observed_generated\n",
    "\n",
    "#     avg_gen = mean(gen_times) if gen_times else float(\"nan\")\n",
    "#     tokens_per_sec = (observed_generated / avg_gen) if avg_gen and avg_gen > 0 else float(\"inf\")\n",
    "\n",
    "#     results = {\n",
    "#         \"ttf_s\": float(ttf),\n",
    "#         \"ttf_generated_tokens\": int(first_generated),\n",
    "#         \"avg_gen_time_s\": float(avg_gen),\n",
    "#         \"tokens_generated\": int(observed_generated),\n",
    "#         \"tokens_per_sec\": float(tokens_per_sec),\n",
    "#         \"gen_times_list\": [float(x) for x in gen_times],\n",
    "#     }\n",
    "\n",
    "#     # minimal print\n",
    "#     print(f\"TTF (generate-only): {results['ttf_s']:.4f}s (generated {results['ttf_generated_tokens']} token(s))\")\n",
    "#     print(f\"Avg generate time: {results['avg_gen_time_s']:.4f}s  |  Tokens/sec (generation-only): {results['tokens_per_sec']:.2f}\")\n",
    "\n",
    "#     return results\n",
    "def generate_text_from_sample(model, processor, sample, max_new_tokens=40, device=\"cuda\"):\n",
    "    inputs = processor.apply_chat_template(\n",
    "        [sample[\"messages\"][1]],\n",
    "        add_generation_prompt=True,\n",
    "        tokenize=True,\n",
    "        return_dict=True,\n",
    "        return_tensors=\"pt\",\n",
    "    ).to(model.device)\n",
    "\n",
    "    outputs = model.generate(**inputs, max_new_tokens=max_new_tokens)\n",
    "    return processor.decode(outputs[0][inputs[\"input_ids\"].shape[-1]:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ghmD0cX9oVoW"
   },
   "outputs": [],
   "source": [
    "# res = generate_text_from_sample(model, processor, sample=train_dataset[0], max_new_tokens=40)\n",
    "# res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mJRt4LdhoVoX"
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import time\n",
    "def clear_memory():\n",
    "    # Delete variables if they exist in the current global scope\n",
    "    if \"inputs\" in globals():\n",
    "        del globals()[\"inputs\"]\n",
    "    if \"model\" in globals():\n",
    "        del globals()[\"model\"]\n",
    "    if \"processor\" in globals():\n",
    "        del globals()[\"processor\"]\n",
    "    if \"trainer\" in globals():\n",
    "        del globals()[\"trainer\"]\n",
    "    if \"peft_model\" in globals():\n",
    "        del globals()[\"peft_model\"]\n",
    "    if \"bnb_config\" in globals():\n",
    "        del globals()[\"bnb_config\"]\n",
    "    time.sleep(2)\n",
    "\n",
    "    # Garbage collection and clearing CUDA memory\n",
    "    gc.collect()\n",
    "    time.sleep(2)\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.synchronize()\n",
    "    time.sleep(2)\n",
    "    gc.collect()\n",
    "    time.sleep(2)\n",
    "\n",
    "    print(f\"GPU allocated memory: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n",
    "    print(f\"GPU reserved memory: {torch.cuda.memory_reserved() / 1024**3:.2f} GB\")\n",
    "\n",
    "\n",
    "# clear_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nySy699tTEyA"
   },
   "source": [
    "# Setting up the training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9bdB2AhkoVoY",
    "outputId": "f058fd7e-86fe-488c-c9ac-6413fec1043b"
   },
   "outputs": [],
   "source": [
    "# Test cell - run this first to check if basic setup works\n",
    "import torch\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"GPU count: {torch.cuda.device_count()}\")\n",
    "for i in range(torch.cuda.device_count()):\n",
    "    print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "2699605e174d4bd48f540d0608b38f21",
      "157444011f584e6db4eb0679400e4d38",
      "2bfee5f918484bc88f880a3b28a60378",
      "6df632ecc46349d1a1201563b9d1f444",
      "85c85e0f92164780af77faf2a7387a6b",
      "ddd6aa960fe4480087cb70a4c91ca810",
      "48d7af150eb14b04aa22ac5ea945461e",
      "58623c91fae04897b8f3f70a4270df72",
      "0cf09c2853a54219847554edf4a22950",
      "65c600b9161e40db87e006737bc1e835",
      "b3485368d53044c8b1a09daf0e5e941d",
      "2eb717da90db4a4aa25b6880d1f72211"
     ]
    },
    "id": "abLV43KgTMAc",
    "outputId": "519f5f39-5b21-442a-a2d5-bbc68758c750"
   },
   "outputs": [],
   "source": [
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    llm_int8_skip_modules=[\"vision_tower\", \"lm_head\"],  # Skip problematic modules\n",
    "    llm_int8_enable_fp32_cpu_offload=True,\n",
    ")\n",
    "\n",
    "\n",
    "from transformers import AutoModelForVision2Seq, AutoProcessor, BitsAndBytesConfig\n",
    "model_id = \"ibm-granite/granite-docling-258M\"\n",
    "\n",
    "model = AutoModelForVision2Seq.from_pretrained(\n",
    "    model_id,\n",
    "    device_map='auto',\n",
    "    torch_dtype=torch.float16,\n",
    "    quantization_config=bnb_config\n",
    ")\n",
    "processor = AutoProcessor.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M2hGJsnwoVoZ",
    "outputId": "ebaab3c1-e017-4398-b389-1ae10091d3d9"
   },
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=8,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    use_dora=True,\n",
    "    init_lora_weights=\"gaussian\",\n",
    ")\n",
    "\n",
    "# don't:\n",
    "# Apply PEFT model adaptation\n",
    "# model = get_peft_model(model, peft_config)\n",
    "# model.add_adapter(peft_config)\n",
    "# model.enable_adapters()\n",
    "\n",
    "# Add adapters only once\n",
    "model = get_peft_model(model, peft_config)\n",
    "\n",
    "# Print trainable parameters\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "Q5a5WK6uoVoa"
   },
   "outputs": [],
   "source": [
    "from trl import SFTConfig\n",
    "training_args = SFTConfig(\n",
    "    output_dir=\"granite-final-finetunned\",\n",
    "    num_train_epochs=3,\n",
    "    # max_steps=30,\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=4,\n",
    "    warmup_steps=1,\n",
    "    learning_rate=1e-4,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=1,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=20,\n",
    "    save_total_limit=1,\n",
    "    optim=\"adamw_torch_fused\",\n",
    "    # bf16=True,\n",
    "    push_to_hub=False,\n",
    "    report_to=\"none\",\n",
    "    remove_unused_columns=False,\n",
    "    gradient_checkpointing=True,\n",
    "    dataset_text_field=\"\",\n",
    "    dataset_kwargs={\"skip_prepare_dataset\": True},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hAKFK_5DoVoa"
   },
   "outputs": [],
   "source": [
    "# import trackio\n",
    "\n",
    "# trackio.init(\n",
    "#     project=\"granite-docling\",\n",
    "#     name=\"granite-docling-trl-sft\",\n",
    "#     config=training_args.to_dict(),\n",
    "#     space_id=training_args.output_dir + \"-trackio\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<|start_of_role|>system<|end_of_role|>A chat between a user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions.<|end_of_text|>\\n<|start_of_role|>user<|end_of_role|><image>Convert this page to docling.<|end_of_text|>\\n<|start_of_role|>assistant<|end_of_role|><doctag><page_header><loc_71><loc_24><loc_85><loc_36>314</page_header>\\n\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Look at your dataset:\n",
    "example = next(iter(train_dataset))\n",
    "texts = processor.apply_chat_template(example, tokenize=False)\n",
    "texts[:400]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The assistant string you are looking for is `<|start_of_role|>assistant<|end_of_role|>`, not `<|assistant|>` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "VO5D9wEaoVoa"
   },
   "outputs": [],
   "source": [
    "def collate_fn(examples):\n",
    "    texts = [processor.apply_chat_template(example, tokenize=False) for example in examples]\n",
    "\n",
    "    image_inputs = []\n",
    "    for example in examples:\n",
    "        image = example[1][\"content\"][0][\"image\"]\n",
    "        if image.mode != \"RGB\":\n",
    "            image = image.convert(\"RGB\")\n",
    "        image_inputs.append([image])\n",
    "\n",
    "    batch = processor(text=texts, images=image_inputs, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "    labels = batch[\"input_ids\"].clone()\n",
    "    # fix assistant text\n",
    "    # assistant_tokens = processor.tokenizer(\"<|assistant|>\", return_tensors=\"pt\")[\"input_ids\"][0]\n",
    "    assistant_tokens = processor.tokenizer(\"<|start_of_role|>assistant<|end_of_role|>\", return_tensors=\"pt\")[\"input_ids\"][0]\n",
    "    eos_token = processor.tokenizer(\"<|end_of_text|>\", return_tensors=\"pt\")[\"input_ids\"][0]\n",
    "\n",
    "    for i in range(batch[\"input_ids\"].shape[0]):\n",
    "        apply_loss = False\n",
    "        for j in range(batch[\"input_ids\"].shape[1]):\n",
    "            if not apply_loss:\n",
    "                labels[i][j] = -100\n",
    "            if (j >= len(assistant_tokens) + 1) and torch.all(\n",
    "                batch[\"input_ids\"][i][j + 1 - len(assistant_tokens) : j + 1] == assistant_tokens\n",
    "            ):\n",
    "                apply_loss = True\n",
    "            if batch[\"input_ids\"][i][j] == eos_token:\n",
    "                apply_loss = False\n",
    "\n",
    "    batch[\"labels\"] = labels\n",
    "\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your collator, check labels\n",
    "\n",
    "train_dataset_iter = iter(train_dataset)\n",
    "samples = [ next(train_dataset_iter) for i in range(8)]\n",
    "batch = collate_fn(samples)\n",
    "\n",
    "if torch.all(batch[\"labels\"] == -100):\n",
    "    print(\"Useless samples!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "OgFhnhacoVob",
    "outputId": "2e7344c7-fc7d-4862-92c0-ea979e883903"
   },
   "outputs": [],
   "source": [
    "from trl import SFTTrainer\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset= test_dataset,\n",
    "    data_collator=collate_fn,\n",
    "    # Don't do this, you added adapters already:\n",
    "    # peft_config=peft_config,\n",
    "    processing_class=processor.tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "BUZYEQGuoVob",
    "outputId": "4c3487d3-35b9-445b-f0cd-8579f2d028a2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='8' max='39' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 8/39 00:50 < 04:20, 0.12 it/s, Epoch 0.56/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.081300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.948400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.141100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.999700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.143800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.064200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[34]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py:2325\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2323\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2324\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2325\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2326\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2327\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2328\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2329\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2330\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py:2618\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2616\u001b[39m update_step += \u001b[32m1\u001b[39m\n\u001b[32m   2617\u001b[39m num_batches = args.gradient_accumulation_steps \u001b[38;5;28;01mif\u001b[39;00m update_step != (total_updates - \u001b[32m1\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m remainder\n\u001b[32m-> \u001b[39m\u001b[32m2618\u001b[39m batch_samples, num_items_in_batch = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_batch_samples\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch_iterator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_batches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2619\u001b[39m \u001b[38;5;66;03m# Store the number of batches for current gradient accumulation\u001b[39;00m\n\u001b[32m   2620\u001b[39m \u001b[38;5;66;03m# This is used to correctly scale the loss when the last accumulation step has fewer batches\u001b[39;00m\n\u001b[32m   2621\u001b[39m \u001b[38;5;28mself\u001b[39m.current_gradient_accumulation_steps = \u001b[38;5;28mlen\u001b[39m(batch_samples)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py:5592\u001b[39m, in \u001b[36mTrainer.get_batch_samples\u001b[39m\u001b[34m(self, epoch_iterator, num_batches, device)\u001b[39m\n\u001b[32m   5590\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_batches):\n\u001b[32m   5591\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m5592\u001b[39m         batch_samples.append(\u001b[38;5;28mnext\u001b[39m(epoch_iterator))\n\u001b[32m   5593\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[32m   5594\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/accelerate/data_loader.py:579\u001b[39m, in \u001b[36mDataLoaderShard.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    577\u001b[39m     current_batch = send_to_device(current_batch, \u001b[38;5;28mself\u001b[39m.device, non_blocking=\u001b[38;5;28mself\u001b[39m._non_blocking)\n\u001b[32m    578\u001b[39m \u001b[38;5;28mself\u001b[39m._update_state_dict()\n\u001b[32m--> \u001b[39m\u001b[32m579\u001b[39m next_batch = \u001b[38;5;28mnext\u001b[39m(dataloader_iter)\n\u001b[32m    580\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m batch_index >= \u001b[38;5;28mself\u001b[39m.skip_batches:\n\u001b[32m    581\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m current_batch\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:733\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    730\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    731\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    732\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m733\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    734\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    735\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    736\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    737\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    738\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    739\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:789\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    787\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    788\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m789\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    790\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    791\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py:55\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n\u001b[32m---> \u001b[39m\u001b[32m55\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[31]\u001b[39m\u001b[32m, line 11\u001b[39m, in \u001b[36mcollate_fn\u001b[39m\u001b[34m(examples)\u001b[39m\n\u001b[32m      8\u001b[39m         image = image.convert(\u001b[33m\"\u001b[39m\u001b[33mRGB\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      9\u001b[39m     image_inputs.append([image])\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m batch = \u001b[43mprocessor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m=\u001b[49m\u001b[43mimage_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpt\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m labels = batch[\u001b[33m\"\u001b[39m\u001b[33minput_ids\u001b[39m\u001b[33m\"\u001b[39m].clone()\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# fix assistant text\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# assistant_tokens = processor.tokenizer(\"<|assistant|>\", return_tensors=\"pt\")[\"input_ids\"][0]\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/transformers/models/idefics3/processing_idefics3.py:292\u001b[39m, in \u001b[36mIdefics3Processor.__call__\u001b[39m\u001b[34m(self, images, text, audio, videos, image_seq_len, **kwargs)\u001b[39m\n\u001b[32m    289\u001b[39m \u001b[38;5;66;03m# Load images if they are URLs\u001b[39;00m\n\u001b[32m    290\u001b[39m images = [[load_image(im) \u001b[38;5;28;01mif\u001b[39;00m is_url(im) \u001b[38;5;28;01melse\u001b[39;00m im \u001b[38;5;28;01mfor\u001b[39;00m im \u001b[38;5;129;01min\u001b[39;00m sample] \u001b[38;5;28;01mfor\u001b[39;00m sample \u001b[38;5;129;01min\u001b[39;00m images]\n\u001b[32m--> \u001b[39m\u001b[32m292\u001b[39m image_inputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mimage_processor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43moutput_kwargs\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mimages_kwargs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    293\u001b[39m inputs.update(image_inputs)\n\u001b[32m    295\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m text \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/transformers/image_processing_utils.py:51\u001b[39m, in \u001b[36mBaseImageProcessor.__call__\u001b[39m\u001b[34m(self, images, **kwargs)\u001b[39m\n\u001b[32m     49\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, images, **kwargs) -> BatchFeature:\n\u001b[32m     50\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Preprocess an image or a batch of images.\"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m51\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpreprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/transformers/models/idefics3/image_processing_idefics3.py:803\u001b[39m, in \u001b[36mIdefics3ImageProcessor.preprocess\u001b[39m\u001b[34m(self, images, do_convert_rgb, do_resize, size, resample, do_image_splitting, do_rescale, max_image_size, rescale_factor, do_normalize, image_mean, image_std, do_pad, return_tensors, return_row_col_info, data_format, input_data_format)\u001b[39m\n\u001b[32m    800\u001b[39m     images_list_cols = [[\u001b[32m0\u001b[39m] * \u001b[38;5;28mlen\u001b[39m(images) \u001b[38;5;28;01mfor\u001b[39;00m images \u001b[38;5;129;01min\u001b[39;00m images_list]\n\u001b[32m    802\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m do_convert_rgb:\n\u001b[32m--> \u001b[39m\u001b[32m803\u001b[39m     images_list = \u001b[43m[\u001b[49m\n\u001b[32m    804\u001b[39m \u001b[43m        \u001b[49m\u001b[43m[\u001b[49m\u001b[43mconvert_to_rgb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpalette\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpalette\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpalettes\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m    805\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpalettes\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mimages_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpalettes_list\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    806\u001b[39m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m    808\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m do_rescale:\n\u001b[32m    809\u001b[39m     images_list = [\n\u001b[32m    810\u001b[39m         [\u001b[38;5;28mself\u001b[39m.rescale(image, rescale_factor, input_data_format=input_data_format) \u001b[38;5;28;01mfor\u001b[39;00m image \u001b[38;5;129;01min\u001b[39;00m images]\n\u001b[32m    811\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m images \u001b[38;5;129;01min\u001b[39;00m images_list\n\u001b[32m    812\u001b[39m     ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/transformers/models/idefics3/image_processing_idefics3.py:804\u001b[39m, in \u001b[36m<listcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m    800\u001b[39m     images_list_cols = [[\u001b[32m0\u001b[39m] * \u001b[38;5;28mlen\u001b[39m(images) \u001b[38;5;28;01mfor\u001b[39;00m images \u001b[38;5;129;01min\u001b[39;00m images_list]\n\u001b[32m    802\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m do_convert_rgb:\n\u001b[32m    803\u001b[39m     images_list = [\n\u001b[32m--> \u001b[39m\u001b[32m804\u001b[39m         \u001b[43m[\u001b[49m\u001b[43mconvert_to_rgb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpalette\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpalette\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpalettes\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m    805\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m images, palettes \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(images_list, palettes_list)\n\u001b[32m    806\u001b[39m     ]\n\u001b[32m    808\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m do_rescale:\n\u001b[32m    809\u001b[39m     images_list = [\n\u001b[32m    810\u001b[39m         [\u001b[38;5;28mself\u001b[39m.rescale(image, rescale_factor, input_data_format=input_data_format) \u001b[38;5;28;01mfor\u001b[39;00m image \u001b[38;5;129;01min\u001b[39;00m images]\n\u001b[32m    811\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m images \u001b[38;5;129;01min\u001b[39;00m images_list\n\u001b[32m    812\u001b[39m     ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/transformers/models/idefics3/image_processing_idefics3.py:804\u001b[39m, in \u001b[36m<listcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m    800\u001b[39m     images_list_cols = [[\u001b[32m0\u001b[39m] * \u001b[38;5;28mlen\u001b[39m(images) \u001b[38;5;28;01mfor\u001b[39;00m images \u001b[38;5;129;01min\u001b[39;00m images_list]\n\u001b[32m    802\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m do_convert_rgb:\n\u001b[32m    803\u001b[39m     images_list = [\n\u001b[32m--> \u001b[39m\u001b[32m804\u001b[39m         [\u001b[43mconvert_to_rgb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpalette\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m img, palette \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(images, palettes)]\n\u001b[32m    805\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m images, palettes \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(images_list, palettes_list)\n\u001b[32m    806\u001b[39m     ]\n\u001b[32m    808\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m do_rescale:\n\u001b[32m    809\u001b[39m     images_list = [\n\u001b[32m    810\u001b[39m         [\u001b[38;5;28mself\u001b[39m.rescale(image, rescale_factor, input_data_format=input_data_format) \u001b[38;5;28;01mfor\u001b[39;00m image \u001b[38;5;129;01min\u001b[39;00m images]\n\u001b[32m    811\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m images \u001b[38;5;129;01min\u001b[39;00m images_list\n\u001b[32m    812\u001b[39m     ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/transformers/models/idefics3/image_processing_idefics3.py:221\u001b[39m, in \u001b[36mconvert_to_rgb\u001b[39m\u001b[34m(image, palette, data_format, input_data_format)\u001b[39m\n\u001b[32m    219\u001b[39m image_rgba = image.convert(\u001b[33m\"\u001b[39m\u001b[33mRGBA\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    220\u001b[39m background = Image.new(\u001b[33m\"\u001b[39m\u001b[33mRGBA\u001b[39m\u001b[33m\"\u001b[39m, image_rgba.size, (\u001b[32m255\u001b[39m, \u001b[32m255\u001b[39m, \u001b[32m255\u001b[39m))\n\u001b[32m--> \u001b[39m\u001b[32m221\u001b[39m alpha_composite = \u001b[43mImage\u001b[49m\u001b[43m.\u001b[49m\u001b[43malpha_composite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbackground\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage_rgba\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    222\u001b[39m alpha_composite = alpha_composite.convert(\u001b[33m\"\u001b[39m\u001b[33mRGB\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    224\u001b[39m output_array = np.array(alpha_composite)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/PIL/Image.py:3555\u001b[39m, in \u001b[36malpha_composite\u001b[39m\u001b[34m(im1, im2)\u001b[39m\n\u001b[32m   3553\u001b[39m im1.load()\n\u001b[32m   3554\u001b[39m im2.load()\n\u001b[32m-> \u001b[39m\u001b[32m3555\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m im1._new(\u001b[43mcore\u001b[49m\u001b[43m.\u001b[49m\u001b[43malpha_composite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim1\u001b[49m\u001b[43m.\u001b[49m\u001b[43mim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mim2\u001b[49m\u001b[43m.\u001b[49m\u001b[43mim\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tfKrWeIgoVob"
   },
   "outputs": [],
   "source": [
    "trainer.save_model(training_args.output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p-YM3C_joVoc"
   },
   "source": [
    "# Testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gPjeeKhxoVoc"
   },
   "outputs": [],
   "source": [
    "clear_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ufcd4licoVoc"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForVision2Seq, AutoProcessor, BitsAndBytesConfig\n",
    "model_id = \"ibm-granite/granite-docling-258M\"\n",
    "model = AutoModelForVision2Seq.from_pretrained(\n",
    "    model_id,\n",
    "    device_map='auto',\n",
    "    torch_dtype=torch.float16,\n",
    ")\n",
    "processor = AutoProcessor.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4pSiwuqqoVoc"
   },
   "outputs": [],
   "source": [
    "adapter_path = \"/kaggle/working/granite-final-finetunned\"\n",
    "model.load_adapter(adapter_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N631-qLioVoc"
   },
   "outputs": [],
   "source": [
    "train_dataset[0][\"images\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Eq6AuFqFoVod"
   },
   "outputs": [],
   "source": [
    "output = generate_text_from_sample(model, processor, train_dataset[0])\n",
    "output"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
