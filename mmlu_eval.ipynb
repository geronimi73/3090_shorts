{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c524ad8e-5534-45d8-9f62-adf9d732dcae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup runpod env\n",
    "# !pip install -U bitsandbytes wandb flash_attn accelerate datasets trl transformers tokenizers peft sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "648db793-1eb5-4c7d-a382-6f27242c71a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "# modelpath=\"mistralai/Mistral-7B-v0.1\"\n",
    "# modelpath=\"alpindale/Mistral-7B-v0.2-hf\"\n",
    "modelpath=\"TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T\"\n",
    "\n",
    "# Load model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    modelpath,    \n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(modelpath, use_fast=False) \n",
    "\n",
    "dataset = load_dataset(\"cais/mmlu\", \"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85b32b28-14ab-4c31-8d58-70908da20d7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 878/878 [05:11<00:00,  2.82it/s, acc=25.02]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc 25.01780373166216\n",
      "total 14042\n",
      "correct 3513\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "letters = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z']\n",
    "choices_tok = [ tokenizer(letters[i],add_special_tokens=False)[\"input_ids\"][-1] for i in range(4) ]\n",
    "\n",
    "# fshots = \"The following are multiple choice questions (with answers) about  abstract algebra.\\n\\nFind all c in Z_3 such that Z_3[x]/(x^2 + c) is a field.\\nA. 0\\nB. 1\\nC. 2\\nD. 3\\nAnswer: B\\n\\nStatement 1 | If aH is an element of a factor group, then |aH| divides |a|. Statement 2 | If H and K are subgroups of G then HK is a subgroup of G.\\nA. True, True\\nB. False, False\\nC. True, False\\nD. False, True\\nAnswer: B\\n\\nStatement 1 | Every element of a group generates a cyclic subgroup of the group. Statement 2 | The symmetric group S_10 has 10 elements.\\nA. True, True\\nB. False, False\\nC. True, False\\nD. False, True\\nAnswer: C\\n\\nStatement 1| Every function from a finite set onto itself must be one to one. Statement 2 | Every subgroup of an abelian group is abelian.\\nA. True, True\\nB. False, False\\nC. True, False\\nD. False, True\\nAnswer: A\"\n",
    "\n",
    "def format_mmlu(entry, include_answer = True):\n",
    "    template = \"{question}\\n{choices}\\nAnswer:\"\n",
    "    choices = [ f\"{letters[i]}. {choice}\" for i, choice in enumerate(entry[\"choices\"])]\n",
    "    choices =  \"\\n\".join(choices)\n",
    "    text = template.format(choices = choices, question = entry[\"question\"])\n",
    "    \n",
    "    if include_answer:\n",
    "        text += f\" {letters[entry['answer']]}\"\n",
    "    return text\n",
    "\n",
    "def mmlu_accuracy(entries, model, tokenizer, batch_size = 8):\n",
    "    mmlu_questions = [format_mmlu(entry, include_answer = False) for entry in entries]\n",
    "    mmlu_batches = [mmlu_questions[i:i + batch_size] for i in range(0, len(mmlu_questions), batch_size)]  \n",
    "    \n",
    "    total, correct = 0, 0    \n",
    "    with tqdm(total=len(mmlu_batches)) as pbar:\n",
    "        for batch_no, batch in enumerate(mmlu_batches):\n",
    "            pbar.update()\n",
    "            batch_tok = tokenizer(batch, return_tensors = \"pt\", padding = True).to(\"cuda\")\n",
    "\n",
    "            with torch.no_grad():\n",
    "                batch_logits = model(**batch_tok).logits\n",
    "                batch_logits.to(\"cpu\")\n",
    "                \n",
    "            for i, logits in enumerate(batch_logits):\n",
    "                # tok_in_len = torch.count_nonzero(batch_tok[\"attention_mask\"][i]).item()\n",
    "                # print(i, tokenizer.decode(batch_tok[\"input_ids\"][i,-1]))\n",
    "                model_choice = torch.argmax(logits[-1][choices_tok]).item()  # 0=batch, -1 is last logit, choices_tok = logits for A, B, C, D\n",
    "                correct += 1 if model_choice == entries[total][\"answer\"] else 0\n",
    "                total += 1\n",
    "            pbar.set_postfix_str(f\"acc={round(correct/total*100,2)}\")\n",
    "    return total, correct, correct/total*100\n",
    "\n",
    "tokenizer.pad_token = tokenizer.unk_token\n",
    "tokenizer.padding_side = \"left\"\n",
    "\n",
    "total, correct, acc = mmlu_accuracy(\n",
    "    # dataset[\"test\"].select(range(1000)),\n",
    "    dataset[\"test\"],\n",
    "    model, \n",
    "    tokenizer,\n",
    "    batch_size = 16\n",
    ")\n",
    "print(\"acc\", acc)\n",
    "print(\"total\", total)\n",
    "print(\"correct\", correct)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
