{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8059103b-a6e0-4b53-bdc8-0ca7b9eb8d79",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%cd /workspace \n",
    "!git clone https://github.com/ostris/ai-toolkit.git\n",
    "!cd ai-toolkit && git submodule update --init --recursive && pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab1be287-fcab-4243-80fb-5a34ae07c3bc",
   "metadata": {},
   "source": [
    "* Get a HuggingFace Token from https://huggingface.co/settings/tokens and paste it in the cell below\n",
    "* Accept FLUX licence: https://huggingface.co/black-forest-labs/FLUX.1-dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ebb9d4-cbda-48a3-b402-61472f154538",
   "metadata": {},
   "outputs": [],
   "source": [
    "!huggingface-cli login --token hf_XXXXXXXX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d78e28af-3bd1-4587-b7dc-82eb8afcd1a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "197636d6-ed14-4629-962d-ee777dff6963",
   "metadata": {},
   "source": [
    "# Upload your images NOW to the folder /workspace/images/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c12e012-1b06-4e90-a199-96b5c96751f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHANGE ME\n",
    "TRIGGER_WORD = \"eksray\"\n",
    "\n",
    "INPUT_FOLDER = \"/workspace/images\"\n",
    "OUTPUT_FOLDER = \"/workspace/output\"\n",
    "LORA_RANK = 16\n",
    "BATCHSIZE = 1\n",
    "LEARNING_RATE = 0.0001\n",
    "STEPS_TRAIN = 3000\n",
    "STEPS_SAVE = 250\n",
    "STEPS_SAMPLE = 250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4803dc10-f13b-4232-a996-4702912cf7f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "job_to_run = OrderedDict([\n",
    "    ('job', 'extension'),\n",
    "    ('config', OrderedDict([\n",
    "        # this name will be the folder and filename name\n",
    "        ('name', 'my_first_flux_lora_v1'),\n",
    "        ('process', [\n",
    "            OrderedDict([\n",
    "                ('type', 'sd_trainer'),\n",
    "                ('training_folder', OUTPUT_FOLDER),\n",
    "                ('performance_log_every', 100),\n",
    "                ('device', 'cuda:0'),\n",
    "                ('trigger_word', TRIGGER_WORD),\n",
    "                ('network', OrderedDict([\n",
    "                    ('type', 'lora'),\n",
    "                    ('linear', LORA_RANK),\n",
    "                    ('linear_alpha', LORA_RANK)\n",
    "                ])),\n",
    "                ('save', OrderedDict([\n",
    "                    ('dtype', 'float16'),  # precision to save\n",
    "                    ('save_every', STEPS_SAVE),  # save every this many steps\n",
    "                    ('max_step_saves_to_keep', 10)  # how many intermittent saves to keep\n",
    "                ])),\n",
    "                ('datasets', [\n",
    "                    # datasets are a folder of images. captions need to be txt files with the same name as the image\n",
    "                    # for instance image2.jpg and image2.txt. Only jpg, jpeg, and png are supported currently\n",
    "                    # images will automatically be resized and bucketed into the resolution specified\n",
    "                    OrderedDict([\n",
    "                        ('folder_path', INPUT_FOLDER),\n",
    "                        ('caption_ext', 'txt'),\n",
    "                        ('caption_dropout_rate', 0.05),  # will drop out the caption 5% of time\n",
    "                        ('shuffle_tokens', False),  # shuffle caption order, split by commas\n",
    "                        ('cache_latents_to_disk', True),  # leave this true unless you know what you're doing\n",
    "                        ('resolution', [512, 768, 1024])  # flux enjoys multiple resolutions\n",
    "                    ])\n",
    "                ]),\n",
    "                ('train', OrderedDict([\n",
    "                    ('batch_size', BATCHSIZE),\n",
    "                    ('steps', STEPS_TRAIN),  # total number of steps to train 500 - 4000 is a good range\n",
    "                    ('gradient_accumulation_steps', 1),\n",
    "                    ('train_unet', True),\n",
    "                    ('train_text_encoder', False),  # probably won't work with flux\n",
    "                    ('content_or_style', 'balanced'),  # content, style, balanced\n",
    "                    ('gradient_checkpointing', True),  # need the on unless you have a ton of vram\n",
    "                    ('noise_scheduler', 'flowmatch'),  # for training only\n",
    "                    ('optimizer', 'adamw8bit'),\n",
    "                    ('lr', LEARNING_RATE),\n",
    "                    # ema will smooth out learning, but could slow it down. Recommended to leave on.\n",
    "                    ('ema_config', OrderedDict([\n",
    "                        ('use_ema', True),\n",
    "                        ('ema_decay', 0.99)\n",
    "                    ])),\n",
    "                    # will probably need this if gpu supports it for flux, other dtypes may not work correctly\n",
    "                    ('dtype', 'bf16')\n",
    "                ])),\n",
    "                ('model', OrderedDict([\n",
    "                    # huggingface model name or path\n",
    "                    ('name_or_path', 'black-forest-labs/FLUX.1-dev'),\n",
    "                    ('is_flux', True),\n",
    "                    ('quantize', True),  # run 8bit mixed precision\n",
    "                    #('low_vram', True),  # uncomment this if the GPU is connected to your monitors. It will use less vram to quantize, but is slower.\n",
    "                ])),\n",
    "                ('sample', OrderedDict([\n",
    "                    ('sampler', 'flowmatch'),  # must match train.noise_scheduler\n",
    "                    ('sample_every', STEPS_SAMPLE),  # sample every this many steps\n",
    "                    ('width', 1024),\n",
    "                    ('height', 1024),\n",
    "                    ('prompts', [\n",
    "                        # you can add [trigger] to the prompts here and it will be replaced with the trigger word\n",
    "                        'xray of a Boeing 747 [trigger]',\n",
    "                        'xray of a woman holding a coffee cup [trigger]',\n",
    "                        'xray of a dog [trigger]',\n",
    "                    ]),\n",
    "                    ('neg', ''),  # not used on flux\n",
    "                    ('seed', 42),\n",
    "                    ('walk_seed', True),\n",
    "                    ('guidance_scale', 4),\n",
    "                    ('sample_steps', 20)\n",
    "                ]))\n",
    "            ])\n",
    "        ])\n",
    "    ])),\n",
    "    # you can add any additional meta info here. [name] is replaced with config name at top\n",
    "    ('meta', OrderedDict([\n",
    "        ('name', '[name]'),\n",
    "        ('version', '1.0')\n",
    "    ]))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2517ea72-73f0-4052-8651-e0a92fb88503",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append('/workspace/ai-toolkit')\n",
    "from toolkit.job import run_job\n",
    "\n",
    "run_job(job_to_run)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
